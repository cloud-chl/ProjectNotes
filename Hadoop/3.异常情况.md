### 1、data下数据被删除，导致namenode数据也被删除，无法启动服务
```markdown
1. 删除每个节点下的 data logs目录
2. 重新格式化 hdfs namenode -format
3. 启动集群 dfs-start.sh
```

### 2、resourcemanager无法完成资源的调度，导致nodemanager无法启动container
```markdown
# 报错日志
java.io.IOException: Could not find a good work dir /home/shsnc/snc_product/hadoop/data/tmp/nm-local-dir/usercache/hdfs/appcache/application_1749524722166_0002/container_1749524722166_0002_01_000002 for container container_1749524722166_0002_01_000002

# 排查过程
1.访问hadoop:8088端口，可以看到node健康情况，找到unhealthy node，可以看到node异常情况
2.hadoop的/目录使用空间超过阈值，或已使用接近极限时，就会出现这种情况
3.可以考虑扩容hadoop的/目录空间大小
4.通过hadoop fs -du -h /找到那个目录占用了空间最大，并通过hadoop fs -rm -r /test删除目录来释放空间
```

![](https://cdn.nlark.com/yuque/0/2025/png/29476003/1749535854219-a3af249b-d25c-48fd-929c-8bf7a1a6b795.png)

![](https://cdn.nlark.com/yuque/0/2025/png/29476003/1749536352885-4bcf55cd-b2b7-4ac4-ac8a-6f2df785f2a5.png)

